# IT5 Dantean Style Transfer

Materials for the final project of the "Human Language Technologies" course (M.Sc. Cognitive Science, UniTn/CIMeC).

The project is to perform Text Style Transfer from standard italian to Dantean italian (medieval italian). Text Style Transfer is a task in which the *source style* (italian) of a text is changed to a chosen *target style* (Dantean). The task aims to change the style of a given sentence while preserving its semantics.

## Dataset

The dataset used for this project is called ita2dante dataset. It contains all the sentences from Dante's "Divine Comedy" along with paraphrases in contemporary italian (approximately 6k pairs in total). The data is scraped from [divinacommedia.weebly.com](https://divinacommedia.weebly.com/). Both the dataset and the notebook used to create it can be found in the folder `dataset`.

Here are the first three rows of the ita2dante dataset:

italian | dante
------------- | -------------
A metà del percorso della vita umana, mi ritrovai per una oscura foresta, poiché avevo smarrito la giusta strada  | Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura, ché la diritta via era smarrita
Ahimè, è difficile descrivere com'era quella foresta, selvaggia, inestricabile e tremenda, tale che al solo pensiero fa tornare la paura | Ahi quanto a dir qual era è cosa dura esta selva selvaggia e aspra e forte che nel pensier rinova la paura
È così spaventosa che la morte lo è poco di più | Tant’è amara che poco è più morte

## Approach

Text Style Transfer is performed here using a supervised approach. Supervised methods adopt the standard neural sequence-to-sequence (seq2seq) model with the encoder-decoder architecture. More precisely, I fine-tuned a T5 Transformer model having sentences in italian as input and original sentences from Dante as output. 

The model used for the fine-tuning is [gsarti/it5-base](https://huggingface.co/gsarti/it5-base) from Giovanni Sarti and Malvina Nissim, released as part of the project ["IT5: Large-Scale Text-to-Text Pretraining for Italian Language Understanding and Generation"](https://arxiv.org/abs/2203.03759).

The notebook used for the fine-tuning is `it5_fine-tuning.ipynb`, the folder `example_results` contain text generated by a model fine-tuned for 7 epochs using [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) optimizer and a learning rate of 5e-4. `output1.csv` contains ouputs from italian paraphrases of the "Divine Comedy" compared with original sentences from Dante, whereas `output2.csv` contains ouputs from random sentences in italian.

Here are three example rows from `output2.csv`:

input | output
------------- | -------------
Ed è nel commosso ricordo di chi non c’è più che cresce il nostro impegno. | Ed è ne la reverenza commossa di chi non c’è più, che cresce l’impegno nostro.
Ma la cosa più assurda era che in questo paese erano state vietate le pasticcerie! | Ma la cosa più assurda era ch’eran le pasticcerie chiuse in questo paese!
Non bisogna riflettere troppo. | Troppo s’ammira.

## Limitations

The biggest limitation for this project is the size of the ita2dante dataset. In fact, it consists only of 6K sentences whereas [gsarti/it5-base](https://huggingface.co/gsarti/it5-base) has more than 200M parameters.

